{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc65e7d-b676-46fa-af71-298a78d7c82d",
   "metadata": {},
   "source": [
    "## æµ‹è¯•ç¯å¢ƒä»£ç†æ˜¯å¦æ­£å¸¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b46af366-a6fa-4bfe-8abd-7a1e85bc4047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HuggingFace è¿æ¥æˆåŠŸï¼ŒçŠ¶æ€ç : 200\n",
      "Test server is ubuntu22.04 GPU 2080Ti 22G\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•ä»£ç†\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "os.environ['ALL_PROXY'] = 'socks5://127.0.0.1:7891'\n",
    "#os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "# æµ‹è¯•ä»£ç†è¿æ¥\n",
    "try:\n",
    "    response = requests.get('https://huggingface.co', timeout=10)\n",
    "    print(\"âœ… HuggingFace è¿æ¥æˆåŠŸï¼ŒçŠ¶æ€ç :\", response.status_code)\n",
    "    print(\"Test server is ubuntu22.04 GPU 2080Ti 22G\")\n",
    "except Exception as e:\n",
    "    print(\"è¿æ¥å¤±è´¥:\", e)\n",
    "\n",
    "# è®¾ç½® HuggingFace ç¼“å­˜è·¯å¾„\n",
    "os.environ['HF_HOME'] = '/home/KevinLiangX/Codes/LLM-quickstart-main/hf'\n",
    "os.environ['HF_HUB_CACHE'] = '/home/KevinLiangX/Codes/LLM-quickstart-main/hf_hu'\n",
    "\n",
    "\n",
    "# æœåŠ¡å™¨ç¯å¢ƒ ubuntu22.04 GPU 2080Ti 22G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2de8ba-d02a-490e-8c2f-7211f333bb69",
   "metadata": {},
   "source": [
    "## è®¾ç½®å…¨å±€å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8279227-8967-416d-abd5-84abf05348e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "model_dir = \"models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "batch_size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a712794f-f33f-469e-9d03-3d1a58ab878b",
   "metadata": {},
   "source": [
    "## å‡†å¤‡æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69628c4f-e23f-4618-947a-5b853c22462a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ åŠ è½½æ•°æ®é›†...\n",
      "âœ… å…¨éƒ¨åŠ è½½å®Œæˆ: è®­ç»ƒ29056 éªŒè¯10581\n"
     ]
    }
   ],
   "source": [
    "# ## ä¼˜åŒ–çš„æ•°æ®é›†åŠ è½½æ–¹æ¡ˆ\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# è®¾ç½®ç¼“å­˜ç›®å½•ï¼Œé¿å…é‡å¤ä¸‹è½½\n",
    "cache_dir = \"/home/KevinLiangX/Codes/LLM-quickstart-main/hf_cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "def load_dataset_with_cache(dataset_name, language_abbr, split, cache_dir):\n",
    "    \"\"\"å¸¦ç¼“å­˜çš„æ•°æ®é›†åŠ è½½ï¼Œé¿å…é‡å¤ä¸‹è½½\"\"\"\n",
    "    try:\n",
    "        return load_dataset(\n",
    "            dataset_name, \n",
    "            language_abbr, \n",
    "            split=split, \n",
    "            trust_remote_code=True,\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"åŠ è½½{split}å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"ğŸ“¥ åŠ è½½æ•°æ®é›†...\")\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "# åˆ†åˆ«åŠ è½½ï¼Œå¤±è´¥æ—¶å¯ä»¥å•ç‹¬é‡è¯•\n",
    "common_voice[\"train\"] = load_dataset_with_cache(dataset_name, language_abbr, \"train\", cache_dir)\n",
    "common_voice[\"validation\"] = load_dataset_with_cache(dataset_name, language_abbr, \"validation\", cache_dir)\n",
    "\n",
    "# æ£€æŸ¥åŠ è½½ç»“æœ\n",
    "if all(v is not None for v in common_voice.values()):\n",
    "    print(f\"âœ… å…¨éƒ¨åŠ è½½å®Œæˆ: è®­ç»ƒ{len(common_voice['train'])} éªŒè¯{len(common_voice['validation'])}\")\n",
    "else:\n",
    "    print(\"âŒ éƒ¨åˆ†æ•°æ®é›†åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451211bf-4d68-482e-ae13-4422fb006ff7",
   "metadata": {},
   "source": [
    "## é¢„å¤„ç†è®­ç»ƒæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7408a1a-3bcf-4b82-aa6a-3ba4c3867f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7f6917c-a63d-42a1-b0c4-dff9c667ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "\n",
    "## ç§»é™¤æ•°æ®é›†ä¸­ä¸å¿…è¦çš„å­—æ®µ\n",
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "\n",
    "## é™ä½éŸ³é¢‘æ•°æ®çš„é‡‡æ ·16kHz\n",
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a0575-f995-43a2-8678-b0c852bf74da",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨å…¨é‡æ•°æ®è¿›è¡Œè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5208c6-437d-4648-a191-61b1534c0144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9faa9bddeea74dcc93dc6b89fc001493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10581 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "To support decoding audio data, please install 'torchcodec'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/utils/py_utils.py\", line 586, in _write_generator_to_queue\n    # The import has a comment with 'From:', we'll retrieve it from the given url\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3650, in _map_single\n    def flatten_indices(\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3624, in iter_outputs\n    function=partial(\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3547, in apply_function\n    desc: Optional[str] = None,\n  File \"/tmp/ipykernel_2191832/2988829012.py\", line 7, in prepare_dataset\n    audio = batch[\"audio\"]\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 284, in __getitem__\n    self.keys_to_format.remove(key)\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 379, in format\n    \"\"\"\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 464, in format_column\n    def format_batch(self, pa_table: pa.Table) -> pd.DataFrame:\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 227, in decode_column\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 2116, in decode_column\n    del flattened[column_name]\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 2117, in <listcomp>\n    self = flattened\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 1405, in decode_nested_example\n    n_offsets = reduce(mul, arr.shape[: arr.ndim - i - 1], 1)\n  File \"/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/features/audio.py\", line 172, in decode_example\n    source_url = path.split(\"::\")[-1]\nImportError: To support decoding audio data, please install 'torchcodec'.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_common_voice \u001b[38;5;241m=\u001b[39m \u001b[43mcommon_voice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/dataset_dict.py:946\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    944\u001b[0m     function \u001b[38;5;241m=\u001b[39m bind(function, split)\n\u001b[0;32m--> 946\u001b[0m dataset_dict[split] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[1;32m    968\u001b[0m     function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunc\n",
      "File \u001b[0;32m/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3309\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3306\u001b[0m os\u001b[38;5;241m.\u001b[39menviron \u001b[38;5;241m=\u001b[39m prev_env\n\u001b[1;32m   3307\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3309\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3310\u001b[0m     pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39munprocessed_kwargs_per_job\n\u001b[1;32m   3311\u001b[0m ):\n\u001b[1;32m   3312\u001b[0m     check_if_shard_done(rank, done, content)\n\u001b[1;32m   3314\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/utils/py_utils.py:626\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m--> 626\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/datasets/utils/py_utils.py:626\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m--> 626\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/multiprocess/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mImportError\u001b[0m: To support decoding audio data, please install 'torchcodec'."
     ]
    }
   ],
   "source": [
    "tokenized_common_voice = common_voice.map(prepare_dataset, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c86fa8-75a7-4933-9392-44fc91dd51de",
   "metadata": {},
   "source": [
    "## è‡ªå®šä¹‰è¯­éŸ³æ•°æ®æ•´ç†å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d51c64-e101-48f9-8fed-6ad6702a2737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªé’ˆå¯¹è¯­éŸ³åˆ°æ–‡æœ¬ä»»åŠ¡çš„æ•°æ®æ•´ç†å™¨ç±»\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any  # å¤„ç†å™¨ç»“åˆäº†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨\n",
    "\n",
    "    # æ•´ç†å™¨å‡½æ•°ï¼Œå°†ç‰¹å¾åˆ—è¡¨å¤„ç†æˆä¸€ä¸ªæ‰¹æ¬¡\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # ä»ç‰¹å¾åˆ—è¡¨ä¸­æå–è¾“å…¥ç‰¹å¾ï¼Œå¹¶å¡«å……ä»¥ä½¿å®ƒä»¬å…·æœ‰ç›¸åŒçš„å½¢çŠ¶\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # ä»ç‰¹å¾åˆ—è¡¨ä¸­æå–æ ‡ç­¾ç‰¹å¾ï¼ˆæ–‡æœ¬ä»¤ç‰Œï¼‰ï¼Œå¹¶è¿›è¡Œå¡«å……\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # ä½¿ç”¨-100æ›¿æ¢æ ‡ç­¾ä¸­çš„å¡«å……åŒºåŸŸï¼Œ-100é€šå¸¸ç”¨äºåœ¨æŸå¤±è®¡ç®—ä¸­å¿½ç•¥å¡«å……ä»¤ç‰Œ\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # å¦‚æœæ‰¹æ¬¡ä¸­çš„æ‰€æœ‰åºåˆ—éƒ½ä»¥å¥å­å¼€å§‹ä»¤ç‰Œå¼€å¤´ï¼Œåˆ™ç§»é™¤å®ƒ\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        # å°†å¤„ç†è¿‡çš„æ ‡ç­¾æ·»åŠ åˆ°æ‰¹æ¬¡ä¸­\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch  # è¿”å›æœ€ç»ˆçš„æ‰¹æ¬¡ï¼Œå‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒæˆ–è¯„ä¼°\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b905df-af9c-4096-b03f-ba02b78cdd38",
   "metadata": {},
   "source": [
    "## æ¨¡å‹å‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c9416bb-1894-48bc-bbbc-659045338ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## åŠ è½½é¢„è®­ç»ƒæ¨¡å‹(int8ç²¾åº¦)\n",
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "model.config.forced_decoder_ids = None \n",
    "\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "# è½¬æ¢ç‰¹å®šå‚æ•°int8ä¸º32\n",
    "from peft import prepare_model_for_int8_training\n",
    "\n",
    "model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3def172-daa5-4d8c-b0d6-4c7d272ccbae",
   "metadata": {},
   "source": [
    "##  LoRA Adapteré…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d72c466-247b-43b4-913d-645ff3d0be29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,966,080 || all params: 1,545,271,040 || trainable%: 0.12723204856023188\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªLoraConfigå¯¹è±¡ï¼Œç”¨äºè®¾ç½®LoRAï¼ˆLow-Rank Adaptationï¼‰çš„é…ç½®å‚æ•°\n",
    "config = LoraConfig(\n",
    "    r=4,  # LoRAçš„ç§©ï¼Œå½±å“LoRAçŸ©é˜µçš„å¤§å°\n",
    "    lora_alpha=64,  # LoRAé€‚åº”çš„æ¯”ä¾‹å› å­\n",
    "    # æŒ‡å®šå°†LoRAåº”ç”¨åˆ°çš„æ¨¡å‹æ¨¡å—ï¼Œé€šå¸¸æ˜¯attentionå’Œå…¨è¿æ¥å±‚çš„æŠ•å½±ã€‚\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,  # åœ¨LoRAæ¨¡å—ä¸­ä½¿ç”¨çš„dropoutç‡\n",
    "    bias=\"none\",  # è®¾ç½®biasçš„ä½¿ç”¨æ–¹å¼ï¼Œè¿™é‡Œæ²¡æœ‰ä½¿ç”¨bias\n",
    ")\n",
    "\n",
    "# è·å–PEFTæ¨¡å‹\n",
    "peft_model = get_peft_model(model, config)\n",
    "# æ‰“å° LoRA å¾®è°ƒè®­ç»ƒçš„æ¨¡å‹å‚æ•°ï¼ŒæŸ¥çœ‹å‚ä¸å¾®è°ƒçš„å‚æ•°é‡\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e58be9-6919-4c22-8936-1370b67bf033",
   "metadata": {},
   "source": [
    "## æ¨¡å‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd484b77-9d2a-404e-8160-452443a96bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## è®¾ç½®è¶…å‚\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# è®¾ç½®åºåˆ—åˆ°åºåˆ—æ¨¡å‹è®­ç»ƒçš„å‚æ•°\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,  \n",
    "    per_device_train_batch_size=48, \n",
    "    learning_rate=1e-3,  \n",
    "    num_train_epochs=3,  \n",
    "    \n",
    "    \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    warmup_steps=200,  # åœ¨è®­ç»ƒåˆæœŸå¢åŠ å­¦ä¹ ç‡çš„æ­¥æ•°ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ\n",
    "    fp16=True,  # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¯ä»¥æé«˜è®­ç»ƒé€Ÿåº¦ï¼ŒåŒæ—¶å‡å°‘å†…å­˜ä½¿ç”¨\n",
    "    gradient_accumulation_steps=1,   # æ¢¯åº¦ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿæ›´å¤§batch\n",
    "    per_device_eval_batch_size=64,  # æ¯ä¸ªè®¾å¤‡ä¸Šçš„è¯„ä¼°æ‰¹é‡å¤§å°\n",
    "    dataloader_num_workers=8,       # å¤šè¿›ç¨‹æ•°æ®åŠ è½½\n",
    "    generation_max_length=256,  # ç”Ÿæˆä»»åŠ¡çš„æœ€å¤§é•¿åº¦\n",
    "    logging_steps=20,  # æŒ‡å®šæ—¥å¿—è®°å½•çš„æ­¥éª¤ï¼Œç”¨äºè·Ÿè¸ªè®­ç»ƒè¿›åº¦\n",
    "    remove_unused_columns=False,  # æ˜¯å¦åˆ é™¤ä¸ä½¿ç”¨çš„åˆ—ï¼Œä»¥å‡å°‘æ•°æ®å¤„ç†å¼€é”€\n",
    "    label_names=[\"labels\"],  # æŒ‡å®šæ ‡ç­¾åˆ—çš„åç§°ï¼Œç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­\n",
    "\n",
    "    save_total_limit=3,                   # åªä¿å­˜æœ€æ–°checkpoint\n",
    "    load_best_model_at_end=True,          # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"eval_loss\",    # åŸºäºvalidation lossé€‰æ‹©\n",
    "\n",
    ")\n",
    "\n",
    "# å®ä¾‹åŒ– Seq2SeqTraniner è®­ç»ƒå™¨\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=peft_model,\n",
    "    train_dataset=tokenized_common_voice[\"train\"],\n",
    "    eval_dataset=tokenized_common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a2a6e-fadc-418e-b66d-e59e51a41e61",
   "metadata": {},
   "source": [
    "## å¯åŠ¨è®­ç»ƒ å¹¶ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3af2baa3-6d3b-4fb5-a6b5-9ea97d32d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1818' max='1818' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1818/1818 6:10:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.417392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.389227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.388295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1818, training_loss=0.31751476210204943, metrics={'train_runtime': 22241.1447, 'train_samples_per_second': 3.919, 'train_steps_per_second': 0.082, 'total_flos': 1.85319357677568e+20, 'train_loss': 0.31751476210204943, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¯åŠ¨è®­ç»ƒ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07af45a9-8738-43c6-9aa2-da0c7ff2638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜LoRAæ¨¡å‹\n",
    "trainer.save_model(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b936217-0d12-4ac0-b405-a421e96fbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹Loraæ¨¡å‹ä¿¡æ¯\n",
    "peft_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f69b9f-f3e2-40ee-b0ac-78cb5af6ad39",
   "metadata": {},
   "source": [
    "## åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44af47d6-e55f-4143-8dd9-0f3c8f7436e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¿®å¤æ¨¡å‹åŠ è½½é—®é¢˜...\n",
      "PEFTé…ç½®: openai/whisper-large-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38fa113d07940b4ba76935baba49270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":\"2025-07-26T04:58:18.936583Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Reqwest(reqwest::Error { kind: Request, source: hyper_util::client::legacy::Error(Connect, Os { code: 32, kind: BrokenPipe, message: \\\"Broken pipe\\\" }) }). Retrying...\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":242}\n",
      "{\"timestamp\":\"2025-07-26T04:58:18.936641Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 2.038532554s before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6a15ff61cb4730b84592413cda5d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨safetensorsæ ¼å¼åŠ è½½æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "language_decode = \"chinese\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoTokenizer, AutoProcessor\n",
    "from peft import PeftConfig, PeftModel\n",
    "import torch\n",
    "\n",
    "print(\"ä¿®å¤æ¨¡å‹åŠ è½½é—®é¢˜...\")\n",
    "\n",
    "# 1. ä»æœ¬åœ°åŠ è½½PEFTé…ç½®\n",
    "peft_config = PeftConfig.from_pretrained(model_dir)\n",
    "print(f\"PEFTé…ç½®: {peft_config.base_model_name_or_path}\")\n",
    "\n",
    "# 2. ä¿®å¤base modelåŠ è½½ - å¤„ç†pytorch_model.binç¼ºå¤±\n",
    "try:\n",
    "    base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        peft_config.base_model_name_or_path, \n",
    "        load_in_8bit=True, \n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,  # æŒ‡å®šæ•°æ®ç±»å‹\n",
    "        use_safetensors=True,       # ä¼˜å…ˆä½¿ç”¨safetensorsæ ¼å¼\n",
    "    )\n",
    "    print(\"ä½¿ç”¨safetensorsæ ¼å¼åŠ è½½æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"safetensorsåŠ è½½å¤±è´¥: {e}\")\n",
    "    print(\"å°è¯•é‡æ–°ä¸‹è½½æ¨¡å‹...\")\n",
    "    \n",
    "    # é‡æ–°ä¸‹è½½å®Œæ•´æ¨¡å‹\n",
    "    base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        peft_config.base_model_name_or_path, \n",
    "        load_in_8bit=True, \n",
    "        device_map=\"auto\",\n",
    "        force_download=True,        # å¼ºåˆ¶é‡æ–°ä¸‹è½½\n",
    "        resume_download=True        # æ”¯æŒæ–­ç‚¹ç»­ä¼ \n",
    "    )\n",
    "    print(\"é‡æ–°ä¸‹è½½å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a04301-930d-424e-9a4e-ce72bdcb56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5e35ee541b42209ac47b79b3d81a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec6084d5c0844c0b20284c3afc1e54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3014aa6d21a4a6192412e4baa1a5028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d5489e893645e19f511e974e7fc285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8896b21acf644688ab9248cf7d24182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c89c13ecfd444286193ba7ff310428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2eb0d77433048e0946f2e20fe036ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dfa9f19c7e4887a6247d89f0255acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰€æœ‰ç»„ä»¶åŠ è½½å®Œæˆ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): WhisperForConditionalGeneration(\n",
       "      (model): WhisperModel(\n",
       "        (encoder): WhisperEncoder(\n",
       "          (conv1): Conv1d(80, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "          (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (embed_positions): Embedding(1500, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperEncoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): WhisperDecoder(\n",
       "          (embed_tokens): Embedding(51865, 1280, padding_idx=50257)\n",
       "          (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x WhisperDecoderLayer(\n",
       "              (self_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): WhisperSdpaAttention(\n",
       "                (k_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=False)\n",
       "                (v_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear8bitLt(\n",
       "                  (base_layer): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear8bitLt(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear8bitLt(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear8bitLt(in_features=5120, out_features=1280, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_out): Linear(in_features=1280, out_features=51865, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ç»§ç»­åŠ è½½LoRAé€‚é…å™¨å’Œå…¶ä»–ç»„ä»¶\n",
    "\n",
    "# 3. åŠ è½½æœ¬åœ°LoRAé€‚é…å™¨\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "print(\"LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ\")\n",
    "\n",
    "# 4. åŠ è½½tokenizerå’Œprocessor\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, \n",
    "    language=language, \n",
    "    task=task\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, \n",
    "    language=language, \n",
    "    task=task\n",
    ")\n",
    "feature_extractor = processor.feature_extractor\n",
    "\n",
    "print(\"æ‰€æœ‰ç»„ä»¶åŠ è½½å®Œæˆ\")\n",
    "\n",
    "# è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "peft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91bc649d-4c37-4199-a541-5c6cf6179eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ›å»ºASR pipelineï¼ˆä¿®å¤deviceå†²çªï¼‰...\n",
      "Pipelineåˆ›å»ºæˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "print(\"åˆ›å»ºASR pipelineï¼ˆä¿®å¤deviceå†²çªï¼‰...\")\n",
    "\n",
    "# ä¸æŒ‡å®šdeviceå‚æ•°ï¼Œè®©accelerateè‡ªåŠ¨ç®¡ç†\n",
    "pipeline = AutomaticSpeechRecognitionPipeline(\n",
    "    model=peft_model, \n",
    "    tokenizer=tokenizer, \n",
    "    feature_extractor=feature_extractor\n",
    "    # ç§»é™¤ device=0 å‚æ•°\n",
    ")\n",
    "\n",
    "# è·å–å¼ºåˆ¶è§£ç ID\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language_decode, task=task)\n",
    "\n",
    "print(\"Pipelineåˆ›å»ºæˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d5b96d2-37e2-41a8-b696-3186d5b5e6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤ æµ‹è¯•å¾®è°ƒæ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾®è°ƒæ¨¡å‹è½¬å½•ç»“æœ:\n",
      "   è¿™æ˜¯ä¸€æ®µæµ‹è¯•ç”¨äºWhisperLarge V2æ¨¡å‹çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æµ‹è¯•ã€‚\n"
     ]
    }
   ],
   "source": [
    "test_audio = \"data/audio/test_zh.flac\"\n",
    "\n",
    "print(\"æµ‹è¯•å¾®è°ƒæ¨¡å‹...\")\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    result = pipeline(\n",
    "        test_audio, \n",
    "        max_new_tokens=255,\n",
    "        generate_kwargs={\n",
    "            \"language\": \"chinese\", \n",
    "            \"task\": \"transcribe\"   \n",
    "        }\n",
    "    )\n",
    "    text = result[\"text\"]\n",
    "\n",
    "print(f\"å¾®è°ƒæ¨¡å‹è½¬å½•ç»“æœ:\")\n",
    "print(f\"   {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4ffa8-7a7f-4df6-a73f-380aedc7e960",
   "metadata": {},
   "source": [
    "## OpenAI Whisper LoRA æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fcf6ea3-ca96-4541-89b2-66129b8ed0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "model_dir = \"models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "batch_size=16\n",
    "\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoTokenizer, AutoProcessor\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_dir)\n",
    "\n",
    "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "base_model.requires_grad_(False)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_dir)\n",
    "peft_model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "processor = AutoProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "feature_extractor = processor.feature_extractor\n",
    "print(\"å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f796ff6-0336-435c-a7a1-a161b75a33b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since mozilla-foundation/common_voice_11_0 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'zh-CN' at /home/KevinLiangX/Codes/LLM-quickstart-main/hf_cache/mozilla-foundation___common_voice_11_0/zh-CN/11.0.0/3f27acf10f303eac5b6fbbbe02495aeddb46ecffdb0a2fe3507fcfbf89094631 (last modified on Sat Jul 26 15:05:05 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰: 10581 æ ·æœ¬\n"
     ]
    }
   ],
   "source": [
    "# ä¼˜åŒ–æ•°æ®é›†åŠ è½½ - ä½¿ç”¨ç¼“å­˜ï¼Œé¿å…é‡æ–°ä¸‹è½½\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "\n",
    "# è®¾ç½®ç¼“å­˜ç›®å½•ï¼Œä½¿ç”¨ä¹‹å‰è®­ç»ƒæ—¶çš„ç¼“å­˜\n",
    "cache_dir = \"/home/KevinLiangX/Codes/LLM-quickstart-main/hf_cache\"  # ä½¿ç”¨ä¹‹å‰çš„ç¼“å­˜ç›®å½•\n",
    "\n",
    "print(\"åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰...\")\n",
    "\n",
    "def load_dataset_with_cache(dataset_name, language_abbr, split, cache_dir):\n",
    "    \"\"\"ä½¿ç”¨ç¼“å­˜åŠ è½½æ•°æ®é›†ï¼Œé¿å…é‡æ–°ä¸‹è½½\"\"\"\n",
    "    try:\n",
    "        return load_dataset(\n",
    "            dataset_name, \n",
    "            language_abbr, \n",
    "            split=split, \n",
    "            trust_remote_code=True,\n",
    "            cache_dir=cache_dir  # ä½¿ç”¨å·²æœ‰ç¼“å­˜\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"åŠ è½½{split}å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "# åªåŠ è½½æµ‹è¯•é›†ï¼Œä½¿ç”¨ç¼“å­˜\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"test\"] = load_dataset_with_cache(dataset_name, language_abbr, \"test\", cache_dir)\n",
    "\n",
    "if common_voice[\"test\"] is not None:\n",
    "    print(f\"æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰: {len(common_voice['test'])} æ ·æœ¬\")\n",
    "else:\n",
    "    print(\"æµ‹è¯•é›†åŠ è½½å¤±è´¥\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76215b57-e038-4847-b01b-8fb732b278b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®å·²é¢„å¤„ç†å®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "tokenized_common_voice = common_voice.map(prepare_dataset)\n",
    "print(\"æ•°æ®å·²é¢„å¤„ç†å®Œæ¯•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "970b0503-0985-4d59-862c-e02cb853ca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®æ•´ç†å™¨å‡†å¤‡å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# æ•°æ®æ•´ç†å™¨ \n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "print(\"æ•°æ®æ•´ç†å™¨å‡†å¤‡å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74bdb255-034a-494c-9bf3-f7414198a105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è§£å†³WERæ¨¡å—é—®é¢˜...\n",
      "\n",
      "é‡æ–°æµ‹è¯•...\n",
      "æ–°çš„evaluateç‰ˆæœ¬: 0.4.1\n",
      " æµ‹è¯•å¤±è´¥: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/spaces?filter=metric (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f9139b5cdc0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\n",
      "\n",
      " ç‰ˆæœ¬ä¿®å¤å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# è§£å†³WERæ¨¡å—ç‰ˆæœ¬é—®é¢˜\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"è§£å†³WERæ¨¡å—é—®é¢˜...\")\n",
    "\n",
    "# 4. é‡æ–°å¯¼å…¥å¹¶æµ‹è¯•\n",
    "print(\"\\né‡æ–°æµ‹è¯•...\")\n",
    "try:\n",
    "    # é‡æ–°å¯¼å…¥evaluate\n",
    "    import importlib\n",
    "    import evaluate\n",
    "    importlib.reload(evaluate)\n",
    "    \n",
    "    print(f\"æ–°çš„evaluateç‰ˆæœ¬: {evaluate.__version__}\")\n",
    "    \n",
    "    # æ£€æŸ¥WERæ˜¯å¦ç°åœ¨å¯ç”¨\n",
    "    available_metrics = evaluate.list_evaluation_modules(module_type=\"metric\")\n",
    "    print(f\" å¯ç”¨æŒ‡æ ‡æ•°é‡: {len(available_metrics)}\")\n",
    "    \n",
    "    if \"wer\" in available_metrics:\n",
    "        print(\" WERç°åœ¨å¯ç”¨äº†ï¼\")\n",
    "        \n",
    "        # æµ‹è¯•åŠ è½½\n",
    "        metric = evaluate.load(\"wer\")\n",
    "        print(\"WERåŠ è½½æˆåŠŸ\")\n",
    "        \n",
    "        # æµ‹è¯•è®¡ç®—\n",
    "        wer_score = metric.compute(predictions=[\"hello\"], references=[\"hello\"])\n",
    "        print(f\" WERè®¡ç®—æˆåŠŸ: {wer_score}\")\n",
    "        \n",
    "    else:\n",
    "        print(\" WERä»ç„¶ä¸å¯ç”¨\")\n",
    "        print(\" æœç´¢åŒ…å«'wer'çš„æŒ‡æ ‡...\")\n",
    "        wer_like = [m for m in available_metrics if 'wer' in m.lower()]\n",
    "        print(f\"åŒ…å«'wer'çš„æŒ‡æ ‡: {wer_like}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" æµ‹è¯•å¤±è´¥: {e}\")\n",
    "\n",
    "print(\"\\n ç‰ˆæœ¬ä¿®å¤å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "206be23f-3333-41f0-8d62-c77ef8f3cd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ä½¿ç”¨å¯ç”¨çš„WERæ¨¡å—...\n",
      " åŠ è½½ argmaxinc/detailed-wer...\n",
      " detailed-wer åŠ è½½å¤±è´¥: Couldn't find a module script at /home/KevinLiangX/Codes/LLM-quickstart-main/peft/argmaxinc/detailed-wer/detailed-wer.py. Module 'argmaxinc/detailed-wer' doesn't exist on the Hugging Face Hub either.\n",
      "\n",
      " WERæ¨¡å—æµ‹è¯•å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨å¯ç”¨çš„detailed-weræ¨¡å—\n",
    "\n",
    "import evaluate\n",
    "\n",
    "print(\" ä½¿ç”¨å¯ç”¨çš„WERæ¨¡å—...\")\n",
    "\n",
    "try:\n",
    "    # ä½¿ç”¨æ‰¾åˆ°çš„detailed-weræ¨¡å—\n",
    "    print(\" åŠ è½½ argmaxinc/detailed-wer...\")\n",
    "    metric = evaluate.load(\"argmaxinc/detailed-wer\")\n",
    "    print(\" detailed-wer åŠ è½½æˆåŠŸï¼\")\n",
    "    \n",
    "    # æµ‹è¯•åŠŸèƒ½\n",
    "    test_predictions = [\"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•\", \"ä½ å¥½ä¸–ç•Œ\"]\n",
    "    test_references = [\"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•\", \"ä½ å¥½ä¸–ç•Œ\"]\n",
    "    \n",
    "    result = metric.compute(predictions=test_predictions, references=test_references)\n",
    "    print(f\" WERè®¡ç®—æˆåŠŸ: {result}\")\n",
    "    \n",
    "    # æ£€æŸ¥è¿”å›çš„ç»“æœæ ¼å¼\n",
    "    print(f\" ç»“æœç±»å‹: {type(result)}\")\n",
    "    print(f\" ç»“æœå†…å®¹: {result}\")\n",
    "    \n",
    "    # å¦‚æœæ˜¯å­—å…¸ï¼ŒæŸ¥çœ‹åŒ…å«çš„é”®\n",
    "    if isinstance(result, dict):\n",
    "        print(f\" å¯ç”¨é”®: {list(result.keys())}\")\n",
    "        \n",
    "        # é€šå¸¸WERåœ¨'wer'é”®ä¸­\n",
    "        if 'wer' in result:\n",
    "            wer_score = result['wer']\n",
    "            print(f\" WERåˆ†æ•°: {wer_score}\")\n",
    "        elif 'word_error_rate' in result:\n",
    "            wer_score = result['word_error_rate']\n",
    "            print(f\" WERåˆ†æ•°: {wer_score}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" detailed-wer åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "print(\"\\n WERæ¨¡å—æµ‹è¯•å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3833b737-3ceb-44eb-9e95-2edc40bf4515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ä¿®å¤evaluateè·¯å¾„é—®é¢˜...\n",
      " å½“å‰å·¥ä½œç›®å½•: /home/KevinLiangX/Codes/LLM-quickstart-main/peft\n",
      " æ²¡æœ‰å‘ç°å¹²æ‰°çš„æœ¬åœ°ç›®å½•\n",
      " åˆ‡æ¢åˆ°ä¸´æ—¶ç›®å½•: /tmp/tmp_sk0wbe5\n",
      " åœ¨å¹²å‡€ç¯å¢ƒä¸­é‡æ–°åŠ è½½evaluate...\n",
      " å°è¯•ä»HuggingFace HubåŠ è½½WER...\n",
      " å°è¯•åŠ è½½: wer\n",
      " å¤±è´¥: Couldn't find a module script at /tmp/tmp_sk0wbe5/wer/wer.py. Module 'wer' doesn't exist on the Hugging Face Hub either.\n",
      " å°è¯•åŠ è½½: evaluate-metric/wer\n",
      " å¤±è´¥: Couldn't find a module script at /tmp/tmp_sk0wbe5/evaluate-metric/wer/wer.py. Module 'evaluate-metric/wer' doesn't exist on the Hugging Face Hub either.\n",
      " å°è¯•åŠ è½½: huggingface/evaluate-metric-wer\n",
      " å¤±è´¥: Couldn't find a module script at /tmp/tmp_sk0wbe5/huggingface/evaluate-metric-wer/evaluate-metric-wer.py. Module 'huggingface/evaluate-metric-wer' doesn't exist on the Hugging Face Hub either.\n",
      " æ‰€æœ‰WERæ¨¡å—éƒ½åŠ è½½å¤±è´¥\n",
      " æ¢å¤åˆ°åŸç›®å½•: /home/KevinLiangX/Codes/LLM-quickstart-main/peft\n",
      " è·¯å¾„ä¿®å¤æµ‹è¯•å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ä¿®å¤evaluateè·¯å¾„é—®é¢˜\n",
    "\n",
    "import os\n",
    "import evaluate\n",
    "\n",
    "print(\" ä¿®å¤evaluateè·¯å¾„é—®é¢˜...\")\n",
    "\n",
    "# 1. æ£€æŸ¥å½“å‰å·¥ä½œç›®å½•\n",
    "current_dir = os.getcwd()\n",
    "print(f\" å½“å‰å·¥ä½œç›®å½•: {current_dir}\")\n",
    "\n",
    "# 2. æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°çš„evaluateæ¨¡å—å¹²æ‰°\n",
    "local_evaluate_dirs = []\n",
    "for item in os.listdir('.'):\n",
    "    if 'evaluate' in item.lower() or 'wer' in item.lower():\n",
    "        local_evaluate_dirs.append(item)\n",
    "\n",
    "if local_evaluate_dirs:\n",
    "    print(f\" å‘ç°å¯èƒ½å¹²æ‰°çš„æœ¬åœ°ç›®å½•: {local_evaluate_dirs}\")\n",
    "else:\n",
    "    print(\" æ²¡æœ‰å‘ç°å¹²æ‰°çš„æœ¬åœ°ç›®å½•\")\n",
    "\n",
    "# 3. ä¸´æ—¶åˆ‡æ¢åˆ°ç³»ç»Ÿä¸´æ—¶ç›®å½•\n",
    "import tempfile\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\" åˆ‡æ¢åˆ°ä¸´æ—¶ç›®å½•: {temp_dir}\")\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "os.chdir(temp_dir)\n",
    "\n",
    "try:\n",
    "    # 4. åœ¨å¹²å‡€ç¯å¢ƒä¸­é‡æ–°å¯¼å…¥evaluate\n",
    "    import importlib\n",
    "    importlib.reload(evaluate)\n",
    "    \n",
    "    print(\" åœ¨å¹²å‡€ç¯å¢ƒä¸­é‡æ–°åŠ è½½evaluate...\")\n",
    "    \n",
    "    # 5. å¼ºåˆ¶ä»Hubä¸‹è½½\n",
    "    print(\" å°è¯•ä»HuggingFace HubåŠ è½½WER...\")\n",
    "    \n",
    "    # è®¾ç½®ç¯å¢ƒå˜é‡ç¡®ä¿åœ¨çº¿æ¨¡å¼\n",
    "    os.environ['HF_DATASETS_OFFLINE'] = '0'\n",
    "    os.environ['HF_EVALUATE_OFFLINE'] = '0'\n",
    "    \n",
    "    # å°è¯•ä¸åŒçš„WERæ¨¡å—\n",
    "    wer_modules = [\n",
    "        \"wer\",\n",
    "        \"evaluate-metric/wer\", \n",
    "        \"huggingface/evaluate-metric-wer\"\n",
    "    ]\n",
    "    \n",
    "    metric = None\n",
    "    for module_name in wer_modules:\n",
    "        try:\n",
    "            print(f\" å°è¯•åŠ è½½: {module_name}\")\n",
    "            metric = evaluate.load(module_name, trust_remote_code=True)\n",
    "            print(f\" æˆåŠŸåŠ è½½: {module_name}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\" å¤±è´¥: {e}\")\n",
    "    \n",
    "    if metric:\n",
    "        # æµ‹è¯•WERè®¡ç®—\n",
    "        test_preds = [\"hello world\"]\n",
    "        test_refs = [\"hello world\"]\n",
    "        wer_score = metric.compute(predictions=test_preds, references=test_refs)\n",
    "        print(f\" WERæµ‹è¯•æˆåŠŸ: {wer_score}\")\n",
    "    else:\n",
    "        print(\" æ‰€æœ‰WERæ¨¡å—éƒ½åŠ è½½å¤±è´¥\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" åœ¨ä¸´æ—¶ç›®å½•ä¸­ä¹Ÿå¤±è´¥: {e}\")\n",
    "\n",
    "finally:\n",
    "    # 6. æ¢å¤åŸç›®å½•\n",
    "    os.chdir(original_dir)\n",
    "    print(f\" æ¢å¤åˆ°åŸç›®å½•: {original_dir}\")\n",
    "\n",
    "print(\" è·¯å¾„ä¿®å¤æµ‹è¯•å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "698b74a4-71a7-4b25-9902-e1c28637d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "## æ”¾å¼ƒ evaluate.werè¯„ä¼°æ–¹å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c09485-08be-4d0b-9b9c-b8ab57aeef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " æµ‹è¯•jiwer...\n",
      " WERè®¡ç®—æˆåŠŸ: 0.0\n",
      " WERç™¾åˆ†æ¯”: 0.00%\n",
      "\n",
      " æµ‹è¯•åŒ…è£…å™¨...\n",
      " æ‰¹æ¬¡æµ‹è¯•: 0.0\n",
      " ç›´æ¥æµ‹è¯•: 0.0\n",
      "\n",
      " jiwerè§£å†³æ–¹æ¡ˆå‡†å¤‡å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n æµ‹è¯•jiwer...\")\n",
    "\n",
    "# æµ‹è¯•æ•°æ®\n",
    "references = [\"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­\", \"ä½ å¥½ä¸–ç•Œ\"]\n",
    "predictions = [\"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­\", \"ä½ å¥½ä¸–ç•Œ\"]\n",
    "\n",
    "# è®¡ç®—WER\n",
    "wer_score = jiwer.wer(references, predictions)\n",
    "print(f\" WERè®¡ç®—æˆåŠŸ: {wer_score}\")\n",
    "print(f\" WERç™¾åˆ†æ¯”: {wer_score * 100:.2f}%\")\n",
    "\n",
    "# 3. åˆ›å»ºå…¼å®¹evaluateæ¥å£çš„åŒ…è£…å™¨\n",
    "class JiwerMetric:\n",
    "    \"\"\"jiweråŒ…è£…å™¨ï¼Œå…¼å®¹evaluateæ¥å£\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.predictions = []\n",
    "        self.references = []\n",
    "    \n",
    "    def add_batch(self, predictions, references):\n",
    "        \"\"\"æ·»åŠ æ‰¹æ¬¡æ•°æ®\"\"\"\n",
    "        self.predictions.extend(predictions)\n",
    "        self.references.extend(references)\n",
    "    \n",
    "    def compute(self, predictions=None, references=None):\n",
    "        \"\"\"è®¡ç®—WER\"\"\"\n",
    "        if predictions is not None and references is not None:\n",
    "            return jiwer.wer(references, predictions)\n",
    "        else:\n",
    "            if not self.predictions or not self.references:\n",
    "                return 0.0\n",
    "            \n",
    "            wer_result = jiwer.wer(self.references, self.predictions)\n",
    "            \n",
    "            # æ¸…ç©ºæ•°æ®\n",
    "            self.predictions = []\n",
    "            self.references = []\n",
    "            \n",
    "            return wer_result\n",
    "\n",
    "# 4. æµ‹è¯•åŒ…è£…å™¨\n",
    "print(\"\\n æµ‹è¯•åŒ…è£…å™¨...\")\n",
    "metric = JiwerMetric()\n",
    "\n",
    "# æµ‹è¯•æ‰¹æ¬¡æ·»åŠ \n",
    "metric.add_batch(predictions=[\"æµ‹è¯•1\"], references=[\"æµ‹è¯•1\"])\n",
    "metric.add_batch(predictions=[\"æµ‹è¯•2\"], references=[\"æµ‹è¯•2\"])\n",
    "wer_batch = metric.compute()\n",
    "print(f\" æ‰¹æ¬¡æµ‹è¯•: {wer_batch}\")\n",
    "\n",
    "# æµ‹è¯•ç›´æ¥è®¡ç®—\n",
    "wer_direct = metric.compute(predictions=[\"ç›´æ¥æµ‹è¯•\"], references=[\"ç›´æ¥æµ‹è¯•\"])\n",
    "print(f\" ç›´æ¥æµ‹è¯•: {wer_direct}\")\n",
    "\n",
    "print(\"\\n jiwerè§£å†³æ–¹æ¡ˆå‡†å¤‡å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44c7bb34-82ce-4f63-b273-d815ec10dd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " å®Œæ•´æµ‹è¯•é›†è¯„ä¼°å‡†å¤‡\n",
      "æµ‹è¯•é›†å¤§å°: 10581\n",
      " æ‰¹æ¬¡å¤§å°: 16\n",
      "æ€»æ‰¹æ¬¡æ•°: 662\n",
      "å¼€å§‹å®Œæ•´æµ‹è¯•é›†è¯„ä¼°...\n",
      "è¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:   0%|                                                                                                                                                     | 0/662 [00:00<?, ?it/s]/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/librosa/core/intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n",
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "å®Œæ•´è¯„ä¼°è¿›åº¦:   0%|â–                                                                                                                                          | 1/662 [00:17<3:18:00, 17.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 16/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:   3%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                     | 21/662 [06:04<3:16:04, 18.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 336/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:   6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                 | 41/662 [11:02<2:25:51, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 656/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                                             | 61/662 [16:11<2:42:36, 16.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 976/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                                         | 81/662 [21:15<2:22:16, 14.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 1296/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                                    | 101/662 [26:43<2:40:25, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 1616/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                | 121/662 [31:51<2:26:57, 16.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 1936/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                           | 141/662 [37:22<2:23:24, 16.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 2256/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                       | 161/662 [42:40<2:16:41, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 2576/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                   | 181/662 [47:58<1:58:46, 14.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 2896/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                               | 201/662 [53:28<2:07:33, 16.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 3216/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                           | 221/662 [58:26<1:56:22, 15.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 3536/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                     | 241/662 [1:03:36<1:51:18, 15.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 3856/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                 | 261/662 [1:08:47<1:41:55, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 4176/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                             | 281/662 [1:14:04<1:35:38, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 4496/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                         | 301/662 [1:19:27<1:32:29, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 4816/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                     | 321/662 [1:25:13<1:32:10, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 5136/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                 | 341/662 [1:30:40<1:29:02, 16.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 5456/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                             | 361/662 [1:35:59<1:23:17, 16.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 5776/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                         | 381/662 [1:41:08<1:12:52, 15.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 6096/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                     | 401/662 [1:46:29<1:05:49, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 6416/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                 | 421/662 [1:51:35<1:06:02, 16.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 6736/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                             | 441/662 [1:56:58<1:03:59, 17.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 7056/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 461/662 [2:02:41<57:36, 17.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 7376/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 481/662 [2:07:53<47:45, 15.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 7696/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 501/662 [2:13:05<44:05, 16.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 8016/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 521/662 [2:18:28<39:16, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 8336/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 541/662 [2:23:40<30:27, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 8656/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 561/662 [2:28:55<26:13, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 8976/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 581/662 [2:34:33<23:57, 17.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 9296/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 601/662 [2:39:49<14:59, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 9616/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 621/662 [2:45:06<10:37, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 9936/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 641/662 [2:50:23<05:46, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 10256/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 661/662 [2:55:41<00:16, 16.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å¤„ç†: 10576/10581 æ ·æœ¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å®Œæ•´è¯„ä¼°è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [2:55:51<00:00, 15.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®Œæ•´æµ‹è¯•é›†è¯„ä¼°æ•°æ®æ”¶é›†å®Œæˆ\n",
      "è®¡ç®—å®Œæ•´æµ‹è¯•é›†WER...\n",
      "\n",
      "======================================================================\n",
      "Whisper LoRA ä¸­æ–‡ASRæ¨¡å‹ - å®Œæ•´æµ‹è¯•é›†è¯„ä¼°ç»“æœ\n",
      "======================================================================\n",
      "æµ‹è¯•æ ·æœ¬æ€»æ•°: 10581\n",
      "è¯é”™è¯¯ç‡ (WER): 68.67%\n",
      "è¯å‡†ç¡®ç‡: 31.33%\n",
      "======================================================================\n",
      "å®Œæ•´æµ‹è¯•é›†è¯„ä¼°å®Œæˆï¼\n",
      "æœ€ç»ˆç»“æœ: {'wer': 0.6867321867321867, 'wer_percent': 68.67321867321867, 'accuracy': 31.32678132678133, 'total_samples': 10581, 'dataset': 'full_test_set'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# å®Œæµ‹è¯•é›†è¯„ä¼°\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "import jiwer\n",
    "\n",
    "# åˆ›å»ºè¯„ä¼°æ•°æ®åŠ è½½å™¨ - ä½¿ç”¨å®Œæ•´æµ‹è¯•é›†\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_common_voice[\"test\"], \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "print(f\" å®Œæ•´æµ‹è¯•é›†è¯„ä¼°å‡†å¤‡\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {len(tokenized_common_voice['test'])}\")\n",
    "print(f\" æ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
    "print(f\"æ€»æ‰¹æ¬¡æ•°: {len(eval_dataloader)}\")\n",
    "\n",
    "# åˆ›å»ºè¯„ä¼°å™¨\n",
    "class FullDatasetWEREvaluator:\n",
    "    def __init__(self):\n",
    "        self.predictions = []\n",
    "        self.references = []\n",
    "    \n",
    "    def add_batch(self, predictions, references):\n",
    "        self.predictions.extend(predictions)\n",
    "        self.references.extend(references)\n",
    "    \n",
    "    def compute(self):\n",
    "        if not self.predictions:\n",
    "            return 0.0\n",
    "        return jiwer.wer(self.references, self.predictions)\n",
    "    \n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            'total_samples': len(self.predictions),\n",
    "            'total_references': len(self.references)\n",
    "        }\n",
    "\n",
    "# åˆå§‹åŒ–è¯„ä¼°å™¨\n",
    "metric = FullDatasetWEREvaluator()\n",
    "\n",
    "print(\"å¼€å§‹å®Œæ•´æµ‹è¯•é›†è¯„ä¼°...\")\n",
    "print(\"è¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n",
    "\n",
    "# è¯„ä¼°å¾ªç¯\n",
    "processed_samples = 0\n",
    "for step, batch in enumerate(tqdm(eval_dataloader, desc=\"å®Œæ•´è¯„ä¼°è¿›åº¦\")):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            # ç”Ÿæˆé¢„æµ‹\n",
    "            generated_tokens = peft_model.generate(\n",
    "                input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                max_new_tokens=255,\n",
    "            ).cpu().numpy()\n",
    "            \n",
    "            # å¤„ç†æ ‡ç­¾\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            \n",
    "            # è§£ç \n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            # æ·»åŠ åˆ°è¯„ä¼°å™¨\n",
    "            metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "            processed_samples += len(decoded_preds)\n",
    "    \n",
    "    # å†…å­˜æ¸…ç†\n",
    "    del generated_tokens, labels, batch\n",
    "    if step % 20 == 0:  # æ›´é¢‘ç¹çš„å†…å­˜æ¸…ç†\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"å·²å¤„ç†: {processed_samples}/{len(tokenized_common_voice['test'])} æ ·æœ¬\")\n",
    "\n",
    "print(\"å®Œæ•´æµ‹è¯•é›†è¯„ä¼°æ•°æ®æ”¶é›†å®Œæˆ\")\n",
    "\n",
    "# è®¡ç®—æœ€ç»ˆWER\n",
    "print(\"è®¡ç®—å®Œæ•´æµ‹è¯•é›†WER...\")\n",
    "wer_score = metric.compute()\n",
    "wer_percent = wer_score * 100\n",
    "stats = metric.get_stats()\n",
    "\n",
    "# æ˜¾ç¤ºç»“æœ\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Whisper LoRA ä¸­æ–‡ASRæ¨¡å‹ - å®Œæ•´æµ‹è¯•é›†è¯„ä¼°ç»“æœ\")\n",
    "print(\"=\"*70)\n",
    "print(f\"æµ‹è¯•æ ·æœ¬æ€»æ•°: {stats['total_samples']}\")\n",
    "print(f\"è¯é”™è¯¯ç‡ (WER): {wer_percent:.2f}%\")\n",
    "print(f\"è¯å‡†ç¡®ç‡: {100-wer_percent:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "print(\"å®Œæ•´æµ‹è¯•é›†è¯„ä¼°å®Œæˆï¼\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "results = {\n",
    "    'wer': wer_score,\n",
    "    'wer_percent': wer_percent,\n",
    "    'accuracy': 100 - wer_percent,\n",
    "    'total_samples': stats['total_samples'],\n",
    "    'dataset': 'full_test_set'\n",
    "}\n",
    "\n",
    "print(f\"æœ€ç»ˆç»“æœ: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d2895-4752-40ef-88cc-f5021b9aeb30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
