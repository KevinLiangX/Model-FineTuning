{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa2559e-4acf-4650-aee5-0bbe5e2e7785",
   "metadata": {},
   "source": [
    "## 测试代理是否正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02bf14ec-5e43-415c-a7a5-836feb151fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "连接失败: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7e62b5ddf280>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\n"
     ]
    }
   ],
   "source": [
    "# 测试代理\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# 设置代理环境变量\n",
    "#os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7890'\n",
    "#os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7890'\n",
    "#os.environ['ALL_PROXY'] = 'socks5://127.0.0.1:7891'\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "# 测试代理连接\n",
    "try:\n",
    "    response = requests.get('https://huggingface.co', timeout=10)\n",
    "    print(\"✅ HuggingFace 连接成功，状态码:\", response.status_code)\n",
    "    print(\"Test server is ubuntu22.04 GPU 2080Ti 22G\")\n",
    "except Exception as e:\n",
    "    print(\"连接失败:\", e)\n",
    "\n",
    "# 设置 HuggingFace 缓存路径\n",
    "os.environ['HF_HOME'] = '/home/KevinLiangX/Codes/LLM-quickstart-main/hf'\n",
    "os.environ['HF_HUB_CACHE'] = '/home/KevinLiangX/Codes/LLM-quickstart-main/hf_hu'\n",
    "\n",
    "\n",
    "# 服务器环境 ubuntu22.04 GPU 2080Ti 22G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e45f4490-36e4-4b7c-97f0-9af570de0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"facebook/opt-6.7b\"\n",
    "\n",
    "quantization_config = GPTQConfig(\n",
    "     bits=4, # 量化精度\n",
    "     group_size=128,\n",
    "     dataset=\"wikitext2\",\n",
    "     desc_act=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0948c78e-1dea-494f-b022-f0718f87b0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1c17cbee4645da842f6072b9fd850a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /home/KevinLiangX/Codes/LLM-quickstart-main/hf/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Fri Aug  1 11:03:43 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ac0046544d41bc9c1488244c811465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.decoder.layers blocks :   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857cc515f74f4a788094617acdf3963c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287358b32e3049d08050a2a52e6905d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a3ead071e84ace99b2f1aa0797f610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038ca8885bf349838481a1caac816391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a93b074fe3741238dda98cb29aabd9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5fc711694b43bcaf0840d43dc9e532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178b510da0b148f399c693bada60970b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5697e37c96c742879b7f6cdb50a75163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f83d97d15784c5bbb962da9224626f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764a0c548f3b4839a1a92bdcae61b785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cceb4ca87ff426c821ce285a5e03d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fc7467b95541d998148ea7048bd6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ee08b0bb83435eb3d7b1ea7eb88463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649d99a8c91f4175b3ceec0cfef43c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ada1e2bf0d4ce1ba94b9d7a4af7f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7abd0b97f504a6dab89b4decaaa9164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cd791a1db24d448836673d9a4e6700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34155446bf9e421f89b297212fb94590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d4c800a4b140c9b6f4a7be51b1e57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebb64ad3d2844d1a99128e09bf0ae2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b79c00f2bc46718b0f9965dce9df06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1abe86092b54d35ae04286fbca17cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0375e865165474eaf2b694558a56ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e87e2192984eb38862b1402ba25aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0aaf4a703de41a9a77f0ef5f40f7be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108d65b0f6104d63b38501271e1745bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c108932ca8a40ad979ba35f31e801ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025446aed2d0405ba2429eb7c7ebebc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6c9f52f7d245438e2f9161c2d44bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b612a9b49cb4ae28b37140b157dede9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5d675d782a447daa28cb92ca1e9fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c38a097f0e4b1d8a08a672d577f588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca68e5b6-c8df-4cd8-897b-6e7861345471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict([('qweight',\n",
       "               tensor([[-1533367701,  2022282410, -1182414405,  ..., -1537760937,\n",
       "                          968190105, -1218869078],\n",
       "                       [-1973762678, -1990888091,  1451669112,  ...,  1767327094,\n",
       "                         1485273242, -1769109111],\n",
       "                       [ -890521657,  1705355194,  2042256023,  ...,  1401453177,\n",
       "                         -963081656, -1212573545],\n",
       "                       ...,\n",
       "                       [-1192544404,   697191045,  1432856694,  ...,  1967820506,\n",
       "                        -1482119368, -1787262823],\n",
       "                       [-1736931943, -1753576812,  2027985786,  ..., -1757504693,\n",
       "                         2090308806, -1987483739],\n",
       "                       [ 1549191317,  1151064006, -1735993498,  ..., -1317428394,\n",
       "                        -1182375288, -1199925157]], device='cuda:0', dtype=torch.int32)),\n",
       "              ('qzeros',\n",
       "               tensor([[2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071],\n",
       "                       ...,\n",
       "                       [2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071],\n",
       "                       [2004318071, 2004318071, 2004318071,  ..., 2004318071, 2004318071,\n",
       "                        2004318071]], device='cuda:0', dtype=torch.int32)),\n",
       "              ('scales',\n",
       "               tensor([[0.0039, 0.0035, 0.0035,  ..., 0.0030, 0.0028, 0.0044],\n",
       "                       [0.0032, 0.0030, 0.0036,  ..., 0.0034, 0.0028, 0.0033],\n",
       "                       [0.0038, 0.0029, 0.0039,  ..., 0.0043, 0.0025, 0.0035],\n",
       "                       ...,\n",
       "                       [0.0033, 0.0034, 0.0033,  ..., 0.0032, 0.0038, 0.0030],\n",
       "                       [0.0030, 0.0028, 0.0027,  ..., 0.0028, 0.0038, 0.0026],\n",
       "                       [0.0034, 0.0035, 0.0034,  ..., 0.0027, 0.0027, 0.0042]],\n",
       "                      device='cuda:0', dtype=torch.float16)),\n",
       "              ('g_idx',\n",
       "               tensor([ 0,  0,  0,  ..., 31, 31, 31], device='cuda:0', dtype=torch.int32)),\n",
       "              ('bias',\n",
       "               tensor([ 0.0061,  0.0129, -0.0215,  ...,  0.0157, -0.0144,  0.0106],\n",
       "                      device='cuda:0', dtype=torch.float16))]),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'infeatures': 4096,\n",
       " 'outfeatures': 4096,\n",
       " 'bits': 4,\n",
       " 'group_size': 128,\n",
       " 'maxq': 15,\n",
       " 'half_indim': 2048,\n",
       " 'use_cuda_fp16': True,\n",
       " 'wf': tensor([[ 0,  4,  8, 12, 16, 20, 24, 28]], dtype=torch.int32),\n",
       " 'kernel_switch_threshold': 128,\n",
       " 'autogptq_cuda_available': False,\n",
       " 'autogptq_cuda': None,\n",
       " 'trainable': False,\n",
       " 'device': device(type='cuda', index=0)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.model.decoder.layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bcfcdcf-71d3-49e6-a73a-5b208618ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model.save_pretrained(\"models/opt-6.7b-gptq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfe3c1e2-349b-460c-919d-125a5d3fd9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/KevinLiangX/Codes/LLM-quickstart-main/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to see you're doing better.\n",
      "Thank you! I'm glad to be back.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "text = \"Merry Christmas! I'm glad to\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = quant_model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9eb6731-0f65-41b8-94a2-f48a0c0300e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e367c7ec58403d8b6c4f913adc1280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b58f4a370a4172bd2b8737c6b372bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.decoder.layers blocks :   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9d5164a9a24f42a5459d72d415ede1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a888c3967b60435899304d5428d65cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5061d6451c9747d1b339a032fb5b8f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6e83bbe5784231af3889f0b1c14e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dcb4bc032b484a9dd774b47f77cf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3882dc74748e46b584e6636084badf7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3785fb25e3a43d28d6f735f76c426d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fc3be0cf564766b05d398f68c666fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1418c2922ca64ba68b8d085797a0f58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d817e6769704e6ebddbdd9bf65852a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd1d3b039e54b2084016b2045bfe95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ef05aacee14df3bbc2c4a3a55e0e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e207ccf57c34dde9ef424de5c1475ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81abaffd53264f30bae0b28b3bd77852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89965b65d1054846a0841cd53b5be22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79485fdabf6248578ba286b79df8295a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21623085dfb478d9e5aa9acb0453531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61c29e254984b5dac70a834a7081bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddd6838ef264c3b93a2b514bfaed367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048c604d65cb414ea491e66a6e49edc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce3e352a42a48758526cfbf70c2148c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8922e3e6b059429e9d9f8d664987fd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b2fe63824b495bb5030310911d017f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5424481bd64876a175852930d881e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2c5a312910476d9e8f1a4359b8b1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575ad0ecd1174c35b6100064fcea943d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab97d8ad0c94d95acfe310bc6d502c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3914d2142f59459c944539c63eeed212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49901b4f9099439faa1aa47d3f74daec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4a2315eb2c426f858ca9f47f319e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d46b01d5c594754be2ff79b044e515b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856eefcef71d43c08447ce02389171cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:optimum.gptq.quantizer:Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, GPTQConfig, AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"facebook/opt-6.7b\"\n",
    "custom_dataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\n",
    "\n",
    "custom_quantization_config = GPTQConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    desc_act=False,\n",
    "    dataset=custom_dataset\n",
    ")\n",
    "\n",
    "custom_quant_model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                                          quantization_config=custom_quantization_config,\n",
    "                                                          torch_dtype=torch.float16,\n",
    "                                                          device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "488a5651-d65f-43c7-a74e-152f51fe1bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to be. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to. to.\n"
     ]
    }
   ],
   "source": [
    "text = \"Merry Christmas! I'm glad to\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = custom_quant_model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54abbece-624b-40f8-a66e-e0dc0a72f380",
   "metadata": {},
   "source": [
    "## 使用 AutoAWQ 量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb949644-2f00-49bc-8f5a-f677cd21ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"facebook/opt-6.7b\"\n",
    "quant_model_dir = \"models/opt-6.7b-awq\"\n",
    "\n",
    "quant_config = {\n",
    "    \"zero_point\": True,\n",
    "    \"q_group_size\": 128,\n",
    "    \"w_bit\": 4,\n",
    "    \"version\": \"GEMM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c61b6dc3-139b-429b-b710-8699503692ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d356884ae46461fbe8c39d38b308bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f56b60770204cde8159567a02a257e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40c5213ed6c48c683f22381a22454ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4c56ab0b694e4cb704c460e6a84eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfb945fb5564147a4c47af417bf0a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb632086-37d1-45cf-baa8-7bb32424f6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ba4e73c13640539f98f8ed522f7425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cea5a097604535884dade4d1508a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469086fbc1d349eead1a5f375c781996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AWQ: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 32/32 [22:12<00:00, 41.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# 量化模型\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b826162-4ac0-487c-a4ba-2363a90122ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "179e9dd2-e554-4f9a-a834-0ec485dd9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# 修改配置文件以使其与transformers集成兼容\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# 预训练的transformers模型存储在model属性中，我们需要传递一个字典\n",
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa676aef-d8c1-4765-b836-98e52fd9f87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/opt-6.7b-awq/tokenizer_config.json',\n",
       " 'models/opt-6.7b-awq/special_tokens_map.json',\n",
       " 'models/opt-6.7b-awq/vocab.json',\n",
       " 'models/opt-6.7b-awq/merges.txt',\n",
       " 'models/opt-6.7b-awq/added_tokens.json',\n",
       " 'models/opt-6.7b-awq/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型权重\n",
    "model.save_quantized(quant_model_dir)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(quant_model_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6701c0ab-fc1f-4ad9-ba07-e27372133f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptAWQForCausalLM(\n",
       "  (model): OPTForCausalLM(\n",
       "    (model): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50272, 4096, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 4096)\n",
       "        (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x OPTDecoderLayer(\n",
       "            (self_attn): OPTAttention(\n",
       "              (k_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=True, w_bit=4, group_size=128)\n",
       "              (v_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=True, w_bit=4, group_size=128)\n",
       "              (q_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=True, w_bit=4, group_size=128)\n",
       "              (out_proj): WQLinear_GEMM(in_features=4096, out_features=4096, bias=True, w_bit=4, group_size=128)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): WQLinear_GEMM(in_features=4096, out_features=16384, bias=True, w_bit=4, group_size=128)\n",
       "            (fc2): WQLinear_GEMM(in_features=16384, out_features=4096, bias=True, w_bit=4, group_size=128)\n",
       "            (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=50272, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbc35719-0704-46b7-ae1a-eadd81c8a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用 GPU 加载量化模型\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(quant_model_dir, device_map=\"cuda\").to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94344e2d-8023-407d-91c6-54e43dad87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57ee1f93-3499-4377-982e-338b3a93b776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to be back happy to be happy merry Christmas! I'm glad to be happy to be happy to be.... happy Christmas!  ^ Christmas, happy happy Merry happy Christmas, Merry Merry happy Christmas to be happy.\n",
      "/r/r/r/r/r/r/r/r/r/r/\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"Merry Christmas! I'm glad to\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d964f83-50b6-4b53-882f-c9aee89be102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman worked as a secretary at the US embassy...\n",
      "\n",
      "The woman worked as a secretary at the US embassy…\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"The woman worked as a\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3f037-8e27-4ecf-afb3-d4651088544d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
